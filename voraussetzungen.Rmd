---
title: "Voraussetzungen im GLM"
bibliography: references.bib
link-citations: yes
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE, message=FALSE}
library(tidyverse)
theme_set(theme_light())

knitr::opts_chunk$set(fig.align = "center", dpi = 350)
```


Wenn wir von Methoden in der Psychologie, oder auch der Medizin reden, meinen wir in den meisten Fällen irgendetwas, das mit dem allgemeinen linearen Modell (**G**enerealized **L**inear **M**odel, kurz **GLM**) zu tun hat. Mit diesem Modell können wir (auch nicht-) lineare Zusammenhänge darstellen und beschreiben. Grunsätzlich nehmen wir vereinfacht an: $$ y = \beta_0 + \beta_1\cdot x + \varepsilon $$

An dieser Stelle verzichten wir mal auf den Index, denn hier geht es um die generelle Idee. Wir versuchen den Wert eines Probanden auf einer abhängigen Variable ($y$) durch eine lineare Kombination (= Addition) von Einflüssen dazustellen. Diese können dann so etwas sein, wie der $y$-Achsen-Abschnitt ($\beta_0$) und die Steigung einer Geraden ($\beta_1$) in Abhängigkeit von einer unabhängigen Variable ($x$). Natürlich ist unser Modell nie perfekt, wir können es immer nur schätzen. Und da das Modell immer eine vereinfachte Form der Wirklichkeit ist,  die Wirklichkeit alos viel komplizierter ist, rechnen wir bereits damit, dass wir Fehler machen, die wir Residuen nennen ($\varepsilon$). Dieses einfache Modell können wir dann um viel mehr Variablen erweitern und transformieren.

Wichtig ist allerdings, dass wir uns, egal wie kompliziert das Modell auch sein mag, immer noch im GLM bewegen. Die Qualität und Teststatistiken des Modells hängen dabei jedoch an bestimmten Voraussetzungen oder Annahmen an das Modell. Bei **parametrischen** Tests haben diese viel mit der Normalverteilung zu tun. Die wichtigsten Annahmen sind dabei

* Additivität und Linearität
* Normalverteilung
* Homoskedastizität
* Unabhängigkeit

Verletzungen dieser Annahmen haben Auswirkungen auf die Schätzung der **Parameter** an sich (die $\beta$s), die **Konfidenzintervalle** dieser Parameter und die **Signifikanztests**. Sie können nämlich schlicht falsch sein -- somit sind die Schlüsse, die wir aus ihnen ziehen, auch falsch.


# Additivität
Die Annahme der Additivität ist relativ simpel. Im GLM (s.o.) haben wir uns dafür entschieden, alle Zusammenhänge als Additionen darzustellen, somit sollten die Daten auch diese Additivität wiederspiegeln. Für simple Modelle bedeutet das: Der Zusammenhang sollte linear sein. Um diese Annahme zu prüfen, sollte man für sich immer explorative Abbildungen erstellen!

```{r echo=FALSE, message=FALSE}
example_data <- tibble(
  x = runif(80, 0, 10),
  fitted_linear = 1.5 * x,
  fitted_quad = x ^ 2,
  error = rnorm(80, sd = 2),
  Linear = fitted_linear + error,
  "Nicht linear" = fitted_quad + error
) %>% 
  pivot_longer(
    cols = Linear:"Nicht linear",
    names_to = "model",
    values_to = "score"
  )

example_data %>% 
  ggplot(aes(x, score)) +
  geom_point() +
  facet_wrap(~ model, scales = "free_y") +
  geom_smooth(method = "lm", se = FALSE) +
  labs(y = "y")
```

Für den linken Datensatz mit jeweils einer kontinuierlichen unabhängigen und abhängigen Variable können wir uns ohne Probleme im GLM bewegen, der Zusammenhang sieht linear aus. Der rechte Datensatz widerum ist überhaupt nicht linear. Wenn wir hier einfach Methoden des GLM zur Auswertung nutzen, werden wir alles, was uns wichtig ist (Parameter, Konfidenzintervalle, Teststatistiken) falsch schätzen.

# Normalverteilung
Mit keiner anderen Voraussetzung oder Annahme wird so viel Schindluder getrieben als mit der Annahme der Normalverteilung. **Nein, die Daten müssen nicht normalverteilt sein!**. Was normalverteilt sein muss, hängt von der Fragestellung ab [@Field.2018].

Wollen wir unsere **Parameter** möglichst gut schätzen (also möglichst ohne bias), dann müssen die Residuen (= Fehler des Modells) normalverteilt sein. Interessierun uns die **Konfidenzintervalle** oder **Signifikanztests**, dann muss die *Stichprobenkennwerteverteilung* normalverteilt sein [@Eid.2017; @Bortz.2010]. Mal wieder so ein Begriff, mit dem die Statistiker uns das Leben schwer machen.

Wir zerlegen den Begriff mal in "Stichprobenkennwert" und "Verteilung". Ein Stichprobenkennwert ist genau das, was es meint, ein Wert, mit dem die Stichprobe beschrieben wird. Am häufigsten benutzen wir da den Mittelwert oder Median der Stichprobe. Deren Verteilung soll normalverteilt sein, was wir nach dem zentralen Grenzwertsatz bei einer hinreichend großen Gruppengröße (ca. $n > 30$) automatisch annehmen können.

Zur Ehrenrettung der Normalverteilungs-Tester: Wenn wir mit kleinen Stichproben ($n < 30$) arbeiten, können wir nicht davon ausgehen, dass der zentrale Grenzwertsatz greift und die Stichprobenkennwerteverteilung automatisch normalverteilt ist. Wenn unsere Daten jedoch normalverteilt sind, könnten wir vermuten, dass auch die Stichprobenkennwerteverteilung normalverteilt ist. Das überprüfen wir dann mit einem QQ-Plot.

```{r echo=FALSE}
tibble(
  "Normalverteilt" = rnorm(300),
  "Nicht normalverteilt" = runif(300, 0, 10)
) %>% 
  pivot_longer(
    cols = everything(),
    names_to = "model",
    values_to = "score",
    names_ptypes = list(model = factor())
  ) %>% 
  ggplot(aes(sample = score)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~ model, scales = "free")
```

Normalverteilte Daten erscheinen in einem QQ-Plot als Punkte auf oder an der eingezeichneten Linie. Im linken Beispiel sieht das sehr normalverteilt aus, im rechten jedoch überhaupt nicht. Um das zu überprüfen gibt es Signifikanztests, wie den Kolmogorov–Smirnov- oder Shapiro–Wilk-Test, die die Alternativhypothese testen, dass die Verteilung der betrachteten Variable nicht normalverteilt ist. Diese Tests kann man jedoch nicht empfehlen, weil sie dieselben Schwächen haben, wie alle anderen Null-Hypothesen-Signifikanztests auch. Erstens ist der  $p$-Wert  abhängig von der Stichprobengröße, das heißt, dass ich mit zunehmender Stichprobengröße eine höherer Wahrscheinlichkeit habe, ein signifikantes (in diesem Falle unerwünschtes Ergebnis) zu erzielen. Selbst super aussehende Verteilungen sind dann laut Test von der Normalverteilung signifkant verschieden. Zweitens heißt ein nicht signifikantes Ergebnis *nicht*, dass die Verteilungen gleich sind [@Goodman.2008].

# Homoskedastizität
Auch bekannt als Varianzhomogenität. Hiermit ist gemeint, dass die Daten für jede Stufe der unabhängigen Variable gleichmäßig streuen. Auch das überprüft man am besten mit einer Abbildung.

```{r echo=FALSE, message=FALSE}
tibble(
  x = runif(200, 0, 10),
  error = rnorm(200, mean = 0, sd = x / 2),
  y = 2 * x + error
) %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

Homoskedastische Daten, also Daten mit Varianzhomogenität hast Du bereits in der linken Abbildung zur Additivität gesehen, in der die Daten gleichmäßig um die blaue Linie streuten. In diesem Fall jedoch sind die Daten richtig schön nicht varianzhomogen. Zu Beginn sind alle Daten eng um die blaue Linie versammelt, aber je größer $x$ wird, desto breiter streuen die Daten. Anders ausgedrückt könnte man auch sagen, dass die Residuen (die Fehler meiner Schätzung = der Abstand der Punkte von der blauen Linie = $\varepsilon$) *abhängig* von meiner unabhängigen Variable sind, und das darf nicht sein!

Gibt es Methoden, Varianzhomogenität auch mit einem Signifikanztest zu überprüfen? Na klar, Tests gibt's für alles, aber auch hier rate nicht nur wieder ich vom Gebrauch ab [@Zimmerman.2004]. Kurz gesagt: Varianzhomogenität ist kein Problem bei gleich großen Gruppen, problemtisch wird's bei verschieden großen Gruppen. Tests, die Varianzhomogenität prüfen, funktionieren am besten bei gleich großen Gruppen und großen Stichproben. Im Umkehrschluss bedeutet das, dass die Tests genau dann nicht richtig funktionieren, wenn es wirklich problematisch wird, also bei ungleichen Gruppengrößen und kleinen Stichproben. In diesen Fällen kann man die Ergebnisse lieber korrigieren, als die Analyse aufgrund von vorläufigen Tests gar nicht durchzuführen.


# Unabhängigkeit
Mit der Unabhängigkeit ist eigentlich die Unabhängigkeit der Fehler ($\varepsilon$) gemeint. Beispielweise sollen die Fehler eines Probanden nicht mit den Fehlern eines anderen Probanden zusammenhängen. Wenn man **unabhängige** Stichproben zieht, ist das in der Regel immer erfüllt; betrachtet man jedoch **verbundene** Stichproben (klassischerweise wie in Designs mit Messwiederholung), sind die Fehler nicht mehr abhängig. Stell dir einmal eine Studie vor, in der Felix und Johanna einen Test zur Stresstoleranz in drei Wochen hintereinander ausgefüllt haben. Du möchtest nun herauskriegen, ob sich der Test-Wert der beiden im Mittel verändert hat. Die Beobachtungen sind aber nicht mehr unabhängig -- im Gegenteil. Felix' Werte sind durch Felix an sich beeinflusst und Johannas Werte von Johanna. Die Werte von Felix und Johanna sind also für sich genommen ähnlicher als die Werte untereinander, weil beide individuelle "Fehler" machen. Wie man damit umgeht, wird auf den entsprechenden Seiten erklärt.

# Robuste Methoden {#robuste_Methoden}
Wenn die Voraussetzungen nicht erfüllt sind, hat da, wie bereits erwähnt, drastische Konsequenzen [@Mair.2020; @Wilcox.2017]. Zum Glück gibt es Rand Wilcox, der viel Zeit seiner Arbeit in **robuste Methoden** investiert hat [@Wilcox.2017]. Das sind Methoden, die keine Annahme an die Normalverteilung oder Varianzhomogenität stellen.

@Wilcox.2017 unterscheidet grob drei robuste Methoden für die von uns oft verwendeten Schätzer (Mittelwert, Standardabweichung, etc.)

* Trimming
* Winsorizing
* M-Schätzer

An dieser Stelle werfe ich deutsche und englische Begriffe durcheinander, weil es nicht immer das Gegenstück in der anderen Sprache gibt.

## Trimming
Beim Trimmen nimmt man sich die Daten, ordnet sich der Größe nach und schneidet dann "oben" und "unten" einen gewissen Prozentsatz ab, z.B. 10%. Übrig bleiben dann 80% der ursprünglichen Daten, allerdings ohne einflussreiche/extrem kleine und große Werte. An dieser Stelle wird bereits deutlich, wie gut diese Methoden beim Vorhandensein von Ausreißern sind.

Den getrimmten Mittelwert kann R von Haus aus berechnen, indem wir der Funktion `mean()` mit `trim = ` angeben, um wie viel die Daten getrimmt werden sollen. Ein guter Start sind 20%. Der Standardfehler des getrimmten Mittelwerts berechnen wir mit `trimse()` aus dem Paket `WRS2` [@Mair.2020]. Trimmt man $n$ Werte um einen bestimmten Prozentsatz, bleiben nach dem Trimmen $h$ Werte (Stichprobengröße nach dem Trimmen) übrig.

```{r}
library(WRS2)

# Zufällige Werte einer schiefen Verteilung
set.seed(20200512)
gruesome_data <- tibble("x" = c(round(rchisq(80, 10)), rep(95:100, times = 2)))

# Getrimmter Mittelwert
gruesome_data %>% 
  summarise(mean = mean(x),
            se = sd(x)/sqrt(n()),
            trimmed_mean = mean(x, trim = 0.2),
            trimmed_se = trimse(x, tr = 0.2))
```

Im direkten Vergleich des klassischen und des robusten Verfahrens fällt auf, dass sich die Werte für Mittelwert und Standardfehler doch schon gut unterscheiden -- die getrimmten Werte sind deutlich kleiner.

## Winsorizing
Das Winsorizing funktioniert ähnlich wie das Trimmen, aber anstatt die obersten und untersten Prozent der Daten abzuschneiden, werden diese durch den jeweils verbleibenden größten, bzw. kleinsten, Wert ersetzt. Den entsprechenden Mittelwert errechnet man mit `winmean()` und den Standardfehler mit `winse()`

```{r}
gruesome_data %>% 
  summarise(mean = mean(x),
            se = sd(x)/sqrt(n()),
            winsorized_mean = winmean(x, tr = 0.2),
            winsorized_se = winse(x, tr = 0.2))
```

## M-Schätzer
M-Schätzer arbeiten auf der Basis von Maximum-Likelihood-Schätzungen [siehe @Wilcox.2017 für weitere Informationen]. Den entsprechenden Mittelwert errechnet man mit `mest()` und den Standardfehler mit `mestse()`.

```{r}
gruesome_data %>% 
  summarise(mean = mean(x),
            se = sd(x)/sqrt(n()),
            m_est_mean = mest(x),
            m_est_se = mestse(x))
```

## Vergleich der robusten Schätzer
Betrachtet man die robusten Schätzer im Vergleich zu den empirischen Daten und der klassischen Methode, stellt man fest, dass die robusten Schätzer ein besseres Bild der Realität wiedergeben.

```{r echo=FALSE, fig.height=3}
tr_mean_se <- function(x) {
  mean_x <- mean(x, trim = 0.2)
  se_x <- trimse(x, tr = 0.2)
  
  tibble("y" = mean_x, "ymin" = mean_x - se_x, "ymax" = mean_x + se_x)
}

win_mean_se <- function(x) {
  mean_x <- winmean(x, tr = 0.2)
  se_x <- winse(x, tr = 0.2)
  
  tibble("y" = mean_x, "ymin" = mean_x - se_x, "ymax" = mean_x + se_x)
}

m_est_mean_se <- function(x) {
  mean_x <- mest(x)
  se_x <- mestse(x)
  
  tibble("y" = mean_x, "ymin" = mean_x - se_x, "ymax" = mean_x + se_x)
}

gruesome_data %>% 
  ggplot(aes(y = x)) +
  geom_jitter(aes(x = 0), width = 0.1, alpha = 0.5) +
  stat_summary(aes(x = 1), fun.data = "mean_se", geom = "pointrange") +
  stat_summary(aes(x = 2), fun.data = "tr_mean_se", geom = "pointrange") +
  stat_summary(aes(x = 3), fun.data = "win_mean_se", geom = "pointrange") +
  stat_summary(aes(x = 4), fun.data = "m_est_mean_se", geom = "pointrange") +
  coord_flip() +
  scale_x_reverse(labels = c("Beobachtet", "Mittelwert", "Getrimmter\nMittelwert", "Winsorized\nMittelwert", "M-Estimator"), breaks = 0:4, minor_breaks = NULL) +
  labs(x = NULL)
```


# Literatur