---
title: "Korrelation"
bibliography: references.bib
link-citations: yes
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE}
knitr::opts_chunk$set(fig.align = "center", dpi = 350)
ggplot2::theme_set(ggplot2::theme_minimal())


library(benelib)
```

![](images/swimmer_r.jpg)

Mit einer Korrelation können wir statistische Zusammenhänge zwischen Variablen aufdecken und angeben, wie sehr diese "zusammenhängen". Wichtig dabei ist, so viel schon einmal vorab, nicht den Zusammenhang an sich (die Korrelation) mit einer Ursache-Wirkungs-Beziehung  (**Kausalität**) gleichzusetzen -- einer der häufigsten und auch von erfahrenen Forschern immer noch begangener Fehler.

# Pakete 
Alle Berechnungen und Abbildungen können wir mit unseren [Standardpaketen](pakete.html) durchführen. Wir benötigen das `tidyverse` zum Data Wrangling und zur Visualisierung der Daten. `haven` benötigen wir für den Import von SPSS-Dateien und `rstatix` für statistische Analysen. Wenn man sich den Import und das Bereinigen der Daten sparen möchte (Schritte, die man dennoch üben sollte), findet man die Daten auch im Paket `costatcompanion`.

```{r message=FALSE}
library(tidyverse)
library(haven)
library(rstatix)
library(costatcompanion)
```

```{r echo=FALSE}
library(patchwork)
```

# Beispiel 1
Wir sind mitten im heißen Sommer, lange Tage, laue Nächte, nach dem Duschen hat man Schweiß auf der Stirn und das letzte Mal, dass Querlüften funktioniert hat, ist schon einige Wochen her. Was könnten wir nun machen, um uns abukühlen? Richtig, wir könnten ins Freibad gehen! Mit einem dahinschmelzenden Calippo in der Hand fragt sich aber der geübte Statistiker: Ist es eigentlich jeden Tag so voll? Oder hängt die Anzahl der Schwimmer in diesem Freibad mit irgendetwas zusammen? Sofort fragt er den Kassenwart nach Daten, Daten, Daten! Und siehe da, tatsächlich führt das Freibad eine Liste darüber, an welchem Tag, wie viele Besucher kamen, inklusive einiger anderer Faktoren, wie z.B. der Durchschnitts-Temperatur.


## Klassisch
Den Datensatz `water_park` finden wir im Paket `costatcompanion`. In der ersten Spalte (`day_id`) finden wir eine individuelle ID des beobachteten Tages (in zufälliger Reihenfolge). Nachfolgend finden wir vier Variablen, die an jedem Tag gemessen wurden. Dazu zählen die Durchschnitts-Temperatur (`temperature`), die Anzahl der Schwimmer im Schwimmbecken (`swimmers`), den Tages-Umsatz des Freibad-Kiosks in Euro (`sales`) und die Anzahl der Schlägereien, wegen der die Polizei gerufen werden musste (`beatings`).

```{r}
costatcompanion::water_park
```

### Voraussetzungen
Da wir uns im GLM bewegen, gelten die üblichen [Voraussetzungen](voraussetzungen.html). Vor allem sollten die Daten jedoch auf Linearität und potenzielle Ausreißer geprüft werden!

### EDA
Zunächst wollen wir untersuchen, ob die Durchschnitts-Temperatur eines beobachteten Tages mit der Anzahl der Schwimmer im Schwimmbecken zusammenhängt. Wir könnten spontan vermuten, dass es mehr Schwimmer bei höheren Temperaturen gibt. 

Eine gute Idee ist, sich deskriptive Statistiken zu seinen untersuchten Variablen auszugeben und sich deren Verteilungen anzuschauen.

```{r}
water_park %>% 
  select(temperature, swimmers) %>% 
  get_summary_stats()
```

```{r echo=FALSE}
water_park %>% 
  select(day_id, temperature, swimmers) %>% 
  pivot_longer(-day_id,
               names_to = "outcome",
               values_to = "score") %>% 
  mutate(outcome = str_to_title(outcome)) %>% 
  ggplot(aes(score, fill = outcome, color = outcome)) +
  geom_density(alpha = 0.5) +
  scale_fill_personal() +
  scale_color_personal() +
  facet_wrap(~ outcome, scales = "free") +
  theme(legend.position = "none") +
  labs(x = "Value", y = "Density")
```

Eine weitere gute Idee wäre, sich einen vermuteten Zusammenhang graphisch als Streudiagramm darstellen zu lassen. Hier finden wir einen guten linearen Zusammenhang, der genau das nahelegt: Je höher die Temperatur (je weiter man auf der $x$-Achse nach rechts geht), desto mehr Schwimmer wurden beobachtet (desto höher liegen die Punkte).

```{r}
water_park %>% 
  ggplot(aes(temperature, swimmers)) +
  geom_point() +
  labs(x = "Temperature", y = "Swimmers") +
  expand_limits(y = 0, x = 0)
```

### Durchführung
Die eigentliche Berechnung einer Korrelation ist einfach und auch der Output hält sich in engen Grenzen:

```{r}
water_park %>% 
  cor_test(temperature, swimmers)
```

Wir erhalten einen statistisch signifikanten Zusammenhang, $r = .860 [.798, .904], p < .001$. Berechnet wird standardmäßig der Korrelationskoeffizient nach Pearson ($r$), auch Produkt-Moment-Korrelation bezeichnet.

### Berichten
We found a significant correlation between an observed day's temperature and the amount of swimmers in the pool, $r = .860 [.798, .904], p < .001$.

## Robust
Natürlich hat Rand @Wilcox.2017 auch für Korrelationen vorgesorgt und stellt uns im Paket `WRS2` [@Mair.2020] robuste Methoden zur Berechnung von Korrelationen zur Verfügung. Einmal gibt es die *percentage bend correlation* ($\rho_{pb}$, `pbcor()`) und einmal die Korrelation auf Basis von winsorized Daten ($\rho_w$, `wincor()`)

```{r}
library(WRS2)

# PB Correlation
with(
  water_park,
  pbcor(temperature, swimmers)
)

# Winsorized Correlation
with(
  water_park,
  wincor(temperature, swimmers)
)
```

Die *percentage bend correlation* beträgt $\rho_{pb} = .823, p < .001$ und die Korrelation basierend auf winsorized Daten $\rho_w = .789, p < .001$.

## Non-parametrisch
Wer [entgegen aller Empfehlungen](voraussetzungen.html) trotzdem non-parametrisch rechnen möchte, der kann auf zwei bekannte Verfahren zurückgreifen. Zum einen den Korrelationskoeffizinten nach Spearman ($r_s$), gelegentlich auch Spearmans Rho ($\rho$) bezeichnet, und zum anderen Kendalls Tau ($\tau$). Nach @Howell.1997 ist $\tau$ zu bevorzugen. Berechnen tun wir beide non-parametrischen Varianten mit derselben Funktion wie Pearsons $r$, geben jedoch unser gewünschtes Verfahren als Metode an.

```{r}
# Spearmans r
water_park %>% 
  cor_test(temperature, swimmers, method = "spearman")

# Kendalls Tau
water_park %>% 
  cor_test(temperature, swimmers, method = "kendall")
```

Eine gute Möglichkeit, sich angesichts der doch unterschiedlichen Ergebnisse nicht zwischen beiden Verfahren im Nachhinein zu entscheiden (Stichwort $p$-Hacking!), ist das vorherige Festlegen des Verfahrens.

# Interpretation
## Varianzaufklärung
Den Korrelationskoeffizienten $r$ kann man qudrieren und erhält $R^2$, das Bestimmtheitsmaß oder Determinationskoeffizient gennant wird -- je nachdem, wie sehr man gerade mit einem statistischen Wissen angeben will. $R^2$ gibt an, wie viel Varianz zwei Variablen miteinander teilen. Ist der Korrelationskoeffizient zwischen zwei Variablen zum Beispiel $r = 0.80$, dann teilen sich diese beiden Variablen aus der Gesamtvarianz der Daten einen Anteil von $R^2 = .80^2 \Leftrightarrow R^2 = 0.64$, also 64%.

## Stärke eines Zusammenhangs
Sehr viele Leute, wenn nicht alle, verwechseln regelmäßig die *Stärke* eines Zusammenhangs mit der *Größe* eines Zusammenhangs. Man kann schnell für sich testen, ob man denselben Fehler machen würde. Betrachten wir die beiden folgenden Abbildungen: In welcher ist die Korrelation größer?

```{r echo=FALSE, message=FALSE}
sample_1 <- MASS::mvrnorm(50, mu = c(0,0), Sigma = matrix(c(1,0.85,0.85,1), ncol = 2), empirical = TRUE) %>% 
  as_tibble(.name_repair = "unique") %>% 
  rename(x = "...1", y = "...2") %>% 
  mutate(sample = "sample_1",
         y = y * 3)

sample_2 <- MASS::mvrnorm(50, mu = c(0,0), Sigma = matrix(c(1,0.70,0.70,1), ncol = 2), empirical = TRUE) %>% 
  as_tibble(.name_repair = "unique") %>% 
  rename(x = "...1", y = "...2") %>% 
  mutate(sample = "sample_2",
         x = x * 0.2)

bind_rows(sample_1, sample_2) %>% 
  mutate(sample = str_to_title(sample), sample = str_replace(sample, pattern = "_", replacement = " ")) %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ sample, scales = "free_x")
```

Wer sich für die linke Seite entschieden hat, ist nicht alleine und liegt gleichzeitig gnadenlos daneben. Die Korrelation ist rechts ($r = .850$) nämlich deutlich höher als links ($r = .700$). Trotzdem entscheiden sich viele für die Linke Abbildung. Wieso? Weil sie eben die Stärke der Korrelation, also die Stärke des Zusammenhangs mit der Größe des Zusammenhangs verwechseln.

Die Stärke eines Zusammenhangs gibt nicht an, wie "steil" die Punkt sind, sondern wie eng sie um eine Linie fallen. Je enger die Punkte um die eingezeichnete Linie fallen, desto größer ist die Korrelation. Wie "steil" die Gerade ist, wie *groß* also der Zusammenhang zwischen zwei Variablen ist, ist eine Frage, auf die die **Regression** Antworten hat.

Was folgt daraus? Die Stärke und die Größe eines Zusammenhangs sind voneinander unabhängig und müssen immer einzeln beurteilt werden. Es kann bspw. sein, dass es Zusammenhänge gibt, die total klein, aber extrem stark sind (wie in der folgenden Abbildung). Hier muss man sich dann die Frage stellen, ob dieser Zusammenhang irgendwelche klinischen/bedeutenden Implikationen mit sich bringt.

```{r echo=FALSE, message=FALSE}
MASS::mvrnorm(500, mu = c(0,0), Sigma = matrix(c(1,0.90,0.90,1), ncol = 2), empirical = TRUE) %>% 
  as_tibble(.name_repair = "unique") %>% 
  rename(x = "...1", y = "...2") %>% 
  mutate(sample = "sample_1",
         y = y * 0.2) %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  expand_limits(y = c(-10, 10))
```

## Korrelation und Kausalität
Nur weil zwei Variablen zusammenhängen, heißt das nicht, dass die eine Variable die andere *kausal*, also ursächlich, beeinflusst. In unserem Beispiel oben haben wir uns den Zusammenhang zwischen Tages-Temperatur und Anzahl von Schwimmern im Schwimmbecken angeschaut. Aber nur, weil wir eine signifikante Korrelation gefunden haben, heißt das *nicht*, dass heißere Tage die Ursache für mehr Schwimmer waren (siehe auch Beispiel 2 unten). Was nun ursächlich für eine Variable oder ein Merkmal ist, kann letzten Endes **nicht durch statistische Verfahren** beurteilt werden, sondern entspringt einer adäquaten **Versuchsplanung** und fällt in den Bereich der experimentellen Methoden (!) Hat man seine Untersuchung nicht darauf ausgelegt, kann man rechnen, was man will, aber man wird nie auf einen kausalen Zusammenhang schließen können.

# Beispiel 2
## Klassisch
Natürlich sind wir nicht darauf beschränkt, uns einzelne Variablen rauszupicken und so einzeln mal eine Korrelation zu berechnen. Wir können das auch über mehrere Variablen gleichzeitig machen. Unser Datensatz `water_park` enthält vier abhängige Variablen, deren Korrelationen wir uns einmal gleichzeitig anschauen können.

```{r}
# Deskriptive Statistiken
water_park %>% 
  select(-day_id) %>% 
  get_summary_stats()

# Korrelationen
water_park %>% 
  select(-day_id) %>% 
  cor_mat()
```

Um nun beurteilen zu können, ob diese Zusammenhänge statistisch signifikant sind, können wir die Funktion `cor_get_pval()` anhängen und erhalten die korrespondierenen $p$-Werte der eben erstellten Korrelationen.

```{r}
water_park %>% 
  select(-day_id) %>% 
  cor_mat() %>% 
  cor_get_pval()
```

Bei insgesamt vier Variablen haben wir noch einen guten Überblick, aber mit wachsendem Datensatz kann dieser Überblick schnell verloren gehen. Eine gute Möglichkeit, sich viele Korrelationen auf einmal anzuschauen liefert ein **Korrelogramm**, das wir mit dem Paket `corrgram` erstellen können.

```{r}
library(corrgram)

outcomes <- water_park %>% 
  select(-day_id) %>% 
  corrgram(upper.panel = panel.conf,
           diag.panel = panel.density)
```

Interessant ist, dass alle Variablen miteinander positiv korrelieren. Und auch hier müssen wir ganz klar in unserer Interpretation sein. Die Variablen hängen zusammen, aber verursachen sich nicht kausal. Denn letzteres würde bedeuten, dass höhere Temperaturen zu mehr Schlägereien führten, höhere Verkaufs-Umsetze des Kiosks zu höheren Temperaturen, oder mehr Schlägereien zu höheren Tages-Umsetzen. Das macht alles nicht wirklich Sinn, und daran kann man gut erkennen, dass Korrelationen keine Kausalitäten liefern.

## Robust
Für *percentage bend correlations* nutzen wir `pball()`, für Korrelationen basierend auf winsorized Daten `winall()`.

```{r}
water_park %>% 
  select(-day_id) %>% 
  pball()

water_park %>% 
  select(-day_id) %>% 
  winall()
```


## Non-parametrisch
Für non-parametrische Tests, von deren Verwendung hier im Zuge der zur Verfügung stehenden robusten Tests wieder entschieden abgeraten wird, gehen wir genau so wie bei der parametrischen Variante vor, aber geben auch hier wieder eine andere Methode als Funktions-Argument mit.

```{r}
# Spearmans r
water_park %>% 
  select(-day_id) %>% 
  cor_mat(method = "spearman")

water_park %>% 
  select(-day_id) %>% 
  cor_mat(method = "spearman") %>% 
  cor_get_pval()

# Kendalls Tau
water_park %>% 
  select(-day_id) %>% 
  cor_mat(method = "kendall")

water_park %>% 
  select(-day_id) %>% 
  cor_mat(method = "kendall") %>% 
  cor_get_pval()
```


# Aus der Praxis
Ein super interessantes Beispiel bescheren uns @Stulp.2013. Sie sammelten Daten zur Größe von US-Präsidenten und untersuchten, ob die Größe eines Präsidentschaftskandidaten einen Einfluss auf das Wählerverhalten der US-Amerikaner hat. Sie untersuchten unter anderem die Hypothese, ob die relative Größe, also das Verhältnis der Größe des gewählten Präsidenten zu der des erfolgreichsten Gegenkandidaten, einen Einfluss auf den *popular vote* hatte. In den USA gibt es das Wahlmänner-System: Die Wähler wählen Wahlmänner (*electoral college*), die dann erst in einem nächsten Schritt den Präsidenten wählen (*electoral vote*). Mit dem *popular vote* bezeichnet man den "eigentlichen" prozentualen Anteil der Wähler, die einen Präsidenten gewählt haben. Wichtig hierbei ist, dass sich *popular vote* und *electoral vote* unterscheiden können -- ein Kanditat muss so nicht mehr als die Hälfte der Wähler (> 50% des *popular vote*) von sich überzeugen, um Präsident zu werden.

## Klassisch
Den Datensatz finden wir in der Excel-Datei `president_heights.xlsx` (zu finden im Ordner `data` des [GitHub-Repositories](https://github.com/benediktclaus/costat/tree/master/data)) und ist eine gekürzte Fassung des von @Stulp.2013 zur Verfügung gestellten Datensatzes. Da es sich hier um eine Excel-Datei handelt, benötigen wir das Paket `readxl` aus dem Tidyverse, um diese Daten einzulesen. Um unseren einheitlichen Stil umzusetzen, bereinigen wir die Variablen-Namen direkt mit der Funktion `clean_names()` aus dem Paket `janitor`.

```{r message=FALSE, warning=FALSE}
# Pakets laden
library(readxl)
library(janitor)

# Daten laden
presidents_heights <- read_excel("data/presidents_heights.xlsx") %>% 
  clean_names()

presidents_heights
```

Wir betrachten hier einen Datensatz mit 61 Zeilen ("Wahlen") und 10 Variablen. In der Variable `president` ist der Name des Präsidenten eingetragen, in `election_year` das Wahljahr, dann die Stelle des gewählten Kandidaten in der Reihenfolge der Präsidenten (`president_number`), die Partei (`party`), die Größe des gewählten Präsidenten (`height`), die Größe des erfolgreichsten Gegenkandidaten (`height_most_popular_opponent`), der *popular vote* für den gewählten Präsidenten (`popular_vote_president`) und selbes für den erfolgreichsten Gegenkandidaten (`popvote_most_popular_opponent`). Die letzten beiden Variablen sind jeweils das Verhältnis der Größe und des *popular vote* von gewähltem Präsidenten und Gegenkandidaten.

Wann immer solche Prozentsätze, Summen oder Mittelwerte gebildet wurden, dürfen wir berechtigterweise skeptisch sein (auch in der Forschung wird so etwas, je nach Kenntnisstand des Ausfüllenden auch noch mal per Hand gemacht) und solche Werte selbst berechnen^[In diesem Fall war jemand fähiges am Werke, denn die Daten stimmen. Wer möchte, kann also darauf verzichten, die Verhältnisse selbst zu berechnen, es ist jedoch eine schöne Übung.]. Das machen wir doch direkt mal: Für die relative Größe des Gewinners teilen wir die Größe des Gewinners durch die des erfolgreichsten Gegenkandidaten. Beim *popular vote* handelt es sich bereits um Prozente, für den relativen Anteil teilen wir also den *popuar vote* des Präsidenten durch die Summe des *popular votes* des Präsidenten und des erfolgreichsten Gegenkandidaten. $$\text{ratio_vote} = \dfrac{\text{vote president}}{\text{vote president} + \text{vote opponent}}$$

```{r}
presidents_heights <- presidents_heights %>% 
  mutate(
    ratio_height = height / height_most_popular_opponent,
    ratio_vote = popular_vote_president / (popular_vote_president + popvote_most_popular_opponent),
  )
```


### EDA
Wie immer, ist es eine gute Idee, die Daten erst einmal zu betrachten, bevor wir anfangen, wilde Modelle zu berechnen. Uns interessiert der Zusammenhang zwischen zwei Variablen: Der relativen Größe des Präsidenten mit dem relativen Anteil am *popular vote*.

```{r}
presidents_heights %>% 
  select(ratio_height, ratio_vote) %>% 
  get_summary_stats()
```

```{r echo=FALSE, warning=FALSE}
presidents_heights %>% 
  select(president, ratio_height, ratio_vote) %>% 
  pivot_longer(
    cols = -president,
    names_to = "outcome",
    names_prefix = "ratio_",
    names_transform = list(outcome = str_to_title),
    values_to = "ratio"
  ) %>% 
  ggplot(aes(ratio, fill = outcome, color = outcome)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~ outcome, scales = "free") +
  scale_fill_personal() +
  scale_color_personal() +
  theme(legend.position = "bottom") +
  labs(x = "Ratio", y = "Density", color = "Variable", fill = "Variable")
```


Lut deskriptiver Statistiken und Abbildungen haben wir keine unplausiblen Werte oder Ausreißer in diesem Datensatz. Betrachten wir anschließend das Streudiagramm der beiden Variablen.

```{r echo=FALSE, warning=FALSE, message=FALSE}
presidents_heights %>% 
  ggplot(aes(ratio_height, ratio_vote)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Height Ratio", y = "Vote Ratio") 
```

Wir können relativ leicht erkennen, dass es einen positiven Zusammenhang zwischen dem Verhältnis der Größen der beiden Kandidaten mit dem Verhältnis des *popular vote* gibt. Je größer ein Kandidat also im Vergleich zu seinem Gegner ist, desto größer ist der Anteil des *popular vote*, den der Kandidat abgreifen kann.

### Durchführung
Die Durchführung ist wie immer kurz und schmerzlos.

```{r}
presidents_heights %>% 
  select(ratio_height, ratio_vote) %>% 
  cor_test()
```

Die sich ergebende Korrelation ist identisch mit der der Autoren [@Stulp.2013, S. 163], $r = .39, p = .007$. An dieser Stelle wieder der Hinweis, dass es sich *nicht* um einen Kausalen Zusammenhang handeln muss. Relativ größere Kandidaten fahren relativ gesehen mehr Stimmen ein, aber wir wissen nicht, ob das tatsächlich *wegen* der Größe so ist.

### Berichten
We found a significant correlation between the height ratio of presidential candidates with their most popular opponent and the ratio of the popular vote, $r = .39, p = .007$.

## Robust
Auch robust können wir einen Zusammenhang feststellen. Wir haben oben zwei Verfahren kennengelernt, die *percentage bend correlation* und die Korrelation basierend auf "winsorizierten" Daten.

```{r}
# Percentage Bend Correlation
with(
  presidents_heights,
  pbcor(ratio_height, ratio_vote)
)

# Winsorized Correlation
with(
  presidents_heights,
  wincor(ratio_height, ratio_vote)
)
```

Die Korrelationen wären, je nach Methode, $\rho_{pb} = .413, p = .004$ (für die *percentage bend correlation*) oder $\rho_w = .402, p = .007$ (für die *winsorized correlation*).

# Literatur
