---
title: "Exploratorische Faktorenanalyse"
bibliography: references.bib
link-citations: yes
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
editor_options: 
  chunk_output_type: console
---

<!-- Only when working on it -->
```{r echo=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.align = "center", dpi = 350)
ggplot2::theme_set(ggplot2::theme_minimal())


library(benelib)
library(patchwork)
library(gt)
```


<!-- Use this after finalizing work -->
<!-- ```{r echo=FALSE, message=FALSE} -->
<!-- knitr::read_chunk("setup.R") -->
<!-- ``` -->

<!-- ```{r echo=FALSE, message=FALSE} -->
<!-- <<setup>> -->
<!-- ``` -->

<!-- Spanner Image -->
![](images/party_r.jpg)

Die explorative Faktorenanalyse nutzen wir, um latente (d.h. nicht beobachtete) Faktoren zu finden, die unseren Daten vermutlich zugrundeliegen. Besonders oft wird dieses Verfahren bei der Erstellung und Validierung von Fragebögen eingesetzt, um zu überprüfen, welche latenten Faktoren mit diesem Fragebogen erfasst werden.

# Pakete 
Alle Berechnungen und Abbildungen können wir mit unseren [Standardpaketen](pakete.html) durchführen. Wir benötigen das `tidyverse` zum Data Wrangling und zur Visualisierung der Daten. `haven` benötigen wir für den Import von SPSS-Dateien und `rstatix` für statistische Analysen. Wenn man sich den Import und das Bereinigen der Daten sparen möchte (Schritte, die man dennoch üben sollte), findet man die Daten auch im Paket `costatcompanion`.

Zudem benötigen wir das Paket `psych`, mit dem wir die Faktorenanalyse durchführen werden.

```{r message=FALSE}
library(tidyverse)
library(haven)
library(rstatix)
library(costatcompanion)
library(psych)
```

# Beispiel
Vielen Leuten macht Feiern einfach Spaß. Deshalb hat ein kluger Kopf einen Fragebogen entwckelt, der abbilden soll, wie sehr sich jemand auf Feiern freut. Der *Fragebogen zur Freude an Festivitäten* (FFF) umfasst 15 Fragen, die auf einer visuellen Analogskala (VAS) auf einer Skala von 0 (keine Zustimmung) bis 10 (vollste Zustimmung) eingeschätzt werden können.

```{r echo=FALSE}
tribble(
  ~ Text,
"Wenn ich an das Wochenende denke, werde ich aufgeregt.", 
"Ich mag es, mich mit anderen Menschen zu unterhalten.", 
"Mich findet man ab 20:00 Uhr auf der Tanzfläche.", 
"Eine spontane Feier wäre jetzt was.", 
"Ich könnte die ganze Nacht durchtanzen.", 
"Andere Menschen sind mir zuwider.", 
"Ich freue mich auf meinen Geburtstag.", 
"Ich freue mich auf die Geburtstage meiner Freunde.", 
"Ich tanze gerne mit anderen Menschen zusammen.", 
"Ich lerne auf Feiern gerne andere Menschen kennen.", 
"Two-Step, Air-Punches und Wave sind meine Sprache.", 
"Ich organisiere gerne Feiern.", 
"Ich freue mich darüber, Einladungen zu Feiern zu erhalten.", 
"Ich feiere gerne mit anderen Menschen.", 
"Es geht mir gut, wenn ich weiß, dass keine Feier ansteht."
) %>% 
  rownames_to_column(var = "Item") %>% 
  gt()
```

Wir wollen nun herausfinden, welche nicht beobachteten Fakoren hinter diesen 15 Fragen, oder auch Items, stecken könnten.

## Voraussetzungen
Wir brauchen Leute, Leute, Leute! Die mit Abstand wichtigste Voraussetzung ist die **Stichprobengröße**, d.h. alles ab 300 ist okay, ab 1000 super -- alles darunter ist wirklich schlech [@Field.2018]. Zudem gibt es das **Kaiser-Meyer-Olkin-Kriterium (KMO)**, das nahe 1 sein sollte. Eine einflussreiche Quelle scheinen hier @Hutcheson.1999 zu sein, die "cut-offs" bezüglich akzeptabler KMO-Werte gebildet haben. Nach ihnen sollte der KMO definitiv größer .70 sein.

Zudem gibt es den **Bartlett-Test**, der die Hypothese prüft, dass die Korrelationsmatrix der Items eine Einheitsmatrix ist. Eine Einheitsmatrix hat auf der Diagonalen Einsen und an allen anderen Stellen Nullen. Eine Korrelationsmatrix, die so aussehen würde, läge nahe, dass die Variablen nicht miteinander korrelieren. Die Basis für eine Faktorenanalyse sind aber Korrelationen.

## Daten
Die Daten sind wie immer im Daten-Ordner des [Git-Repositories](https://github.com/benediktclaus/costat/tree/master/data) (`party.sav`), oder im [`costatcompanion`](pakete.html#costatcompanion) zu finden.

```{r}
party
```

In der ersten Spalte (`id`) sind die Probanden-IDs eingetragen, in den folgenden Spalten die Einschätzung jedes Probanden zu den obigen Aussagen des Fragebogens.

## EDA
Aus den Dimensionen des Datensatzes geht hervor, dass wir eine Stichprobengröße von $N = 320$ haben, was gut ist. Zudem ist es immer eine gute Idee, sich die Korrelationen der Variablen untereinander anzusehen -- am besten in einem Korrelogramm. Zunächst werden wir aber einmal die Spalte `id` los, die zwar für die Dateneingabe sinnvoll und nützlich ist, in den weiteren Analysen aber nur stört.

```{r message=FALSE}
party_raw <- party %>% 
  select(-id)

party_raw

party_raw %>% 
  get_summary_stats()

party_raw %>% 
  corrgram::corrgram(order = TRUE)
```

Uns wird hier auffallen, dass zwei Items (15 und 6) mit allen anderen negativ korrelieren. Das kann passieren, wenn die Items "falsch herum" codiert wurden. In diesem Fall können wir die Annahme bestätigen, denn dies sind die einzigen beiden Fragen, die "entgegen" der Richtung des Fragebogens sind. Alle anderen Fragen bedeuten bei größerer Zustimmung mehr Spaß beim Feiern, diese beiden jedoch nicht. Wir sollten sie also umcodieren.

```{r}
party_recoded <- party_raw %>% 
  mutate(
    across(c(item_6, item_15), ~ 10 - .)
  )

party_recoded %>% 
  corrgram::corrgram(order = TRUE)
```

Und schon sind alle Items miteinander positiv korreliert. Anhand der bereits sichtbaren Clusterung können wir davon ausgehen, dass mindestens ein latenter Faktor vorhanden ist.

### Bartlett-Test
Den Bartlett-Test kann man folgendermaßen ausführen. Das Ergebnis ist signifikant, $K^2(14) = 56.075, p < .001$, und bedeutet, dass unsere Korrelationsmatrix *keine* Einheitsmatrix ist. Also alles super!

```{r}
bartlett.test(party_recoded)
```

### KMO
Auch der KMO ist sehr einfach auszuführen. Unser KMO ist .88, also alles super. Unsere Daten eignen sich also für eine EFA.

```{r}
KMO(party_recoded)
```


## Anzahl der zu extrahierenden Faktoren
### Scree-Plot
Die Situation bei der EFA ist etwas misslich: Wir wollen latente Faktoren "entdecken", müssen dem Verfahren aber vorher sagen, wie viele Faktoren wir eigentlich extrahieren/finden wollen. Deshalb kommt der Bestimmung der Anzahl der zu extrahierenden Faktoren eine besondere Bedeutung zu.

```{r eval=FALSE}
scree(party_recoded, factors = FALSE)
```


```{r echo=FALSE, message=FALSE, results="hide"}
parallel_analysis <- fa.parallel(party_recoded, fa = "pc", plot = FALSE)
```

```{r echo=FALSE}
tibble(
  Eigenvalue = parallel_analysis$pc.values
) %>% 
  rownames_to_column(var = "factor_number") %>% 
  mutate(factor_number = as.numeric(factor_number)) %>% 
  ggplot(aes(factor_number, Eigenvalue, group = 1)) +
  geom_line() +
  geom_point() +
  labs(x = "Factor Number", y = "Eigenvalue")
```

Im Scree-Plot suchen wir nach dem "Knick" in der Linie der Eigenwerte. Nach diesem "Knick" sollen die restlichen Eigenwerte nur noch wie "Geröll" (engl. *scree*) aussehen, also flach ablaufen. Dabei gibt es jedoch unterschiedliche Auffassungen darüber, ob man den letzten Punkt vor dem Knick noch mit aufnimmt, oder nicht. In diesem Beispiel ist das Ergebnis recht klar, wir würden drei Faktoren extrahieren. Beachten sollte man jedoch, dass der Scree-Test ein rein subjektives Verfahren und somit kein wirklicher "Test" ist.

### Parallelanalyse
In SPSS wäre man ohne weiteres Zutun nun aufgeschmissen, in R gibt es zum Glück die Parallelanalyse [@Horn.1965], mit der sich die Anzahl der zu extrahierenden Faktoren objektiv bestimmen lässt. Dazu werden anhand der Dimensionen des Datensatzes zufällige Daten generiert und die dem entsprechenden Eigenwerte in den Scree-Plot eingezeichnet. Extrahieren würden wir so viele Faktoren, deren Eigenwert über denen der zufälligen Simulationen liegt.

```{r eval=FALSE}
fa.parallel(party_recoded, fa = "pc")
```

```{r echo=FALSE}
tibble(
  Actual = parallel_analysis$pc.values,
  Simulated = parallel_analysis$pc.sim,
  Resampled = parallel_analysis$pc.simr
) %>% 
  rownames_to_column(var = "factor_number") %>% 
  pivot_longer(
    cols = -factor_number,
    names_to = "sample",
    values_to = "eigenvalue"
  ) %>% 
  mutate(sample = as_factor(sample),
         factor_number = as.numeric(factor_number)) %>% 
  ggplot(aes(factor_number, eigenvalue, color = sample, group = sample)) +
  geom_line() +
  geom_point() +
  scale_color_personal(reverse = TRUE) +
  labs(x = "Factor Number", y = "Eigenvalue", color = "Sample") +
  theme(legend.position = "bottom")
```

Wir würden also auch anhand des Ergebnisses des Scree-Tests drei Faktoren extrahieren.

## Rotation
Um die Faktoren möglichst gut auseinanderhalten zu können, ist es eine gute Idee, die Koordinatenachsen im $n$-Dimensionalen Raum zu rotieren ($n$ ist dabei die Anzahl der Faktoren). Dabei kann man zwischen **orthogonalen** und **obliquen** Rotationen unterscheiden. Bei orthogonalen Rotationen postuliert man, dass die Faktoren nicht miteinander korrlieren, bei obliquen Rotationen geht man von einer Korrelation der Faktoren aus (wie es in der Psychologie üblich ist).

Gute orthogonale Rotation sind Varimax und Quartimax; gute oblique sind Oblimin und Promax.

## Durchführung
In meiner Benutzung des Begriffs "Faktorenalayse" habe ich bisher etwas geschummelt. Streng genommen machen wir nämlich keine Faktorenanalyse, sondern eine **Hauptkomponentenanalyse**. Bei einer Faktorenanalyse versuchen wir -- ganz vereinfacht gesagt -- latente Faktoren durch die Items zu erklären. Bei der Hauptkomponentenanalyse gehen wir jedoch davon aus, das der Zusammenhang genau anders herum ist: Wie denken uns, dass es irgendwo diese latenten Faktoren gibt und diese dann die Antworten der Probanden auf den Items beeinflussen. Wir schätzen die Faktoren zwar anhand der Items, aber wir unterstellen den Zusammenhang von den latenten Faktoren auf die Items.

Die Hauptkomponentenanalyse führen wir mit der Funktion `principal()` aus. Als Argument `nfactors` geben wir die Anzahl der zu extrahierenden Faktoren ans, als `rotate` die Rotationsmethode.

```{r message=FALSE}
party_efa <- principal(party_recoded, nfactors = 3, rotate = "oblimin")
```

Wir erhalten so einen ganz "messy" Output, in dem uns einiges mitgeteilt wird: Da ist zunächst die Strukturmatrix, in der pro Item die standardisierten Ladungen auf jeden Faktor angegeben sind (sowie die Kommunalitäten in `h2`, Uniqueness in `u2`, und die Komplexität in `com`). Die Faktorladungen nutzen wir, um herauszufinden, auf welchem Faktor ein Item "lädt"; je höher die Ladung, desto mehr "lädt" das Item auf dem Faktor, desto mehr gehört es zu dem Faktor.

Anschließend erhalten wir Angaben zu den extrahierten Faktoren an sich, unter anderem die erklärten Varianzen. Danach gibt es die Korrelation der latenten Faktoren untereinander und noch einige Angaben zur Modellgüte.

Uns interessieren zu Beginn vor allem die Faktorladungen, welche wir uns so ausgeben lassen können.

```{r}
loadings(party_efa)
```

Wir erhalten also die Strukturmatrix von oben, aber schon mit einigen Änderungen. Hier wurden bereits kleine Faktorladungen entfernt. Dieses Verhalten können wir weiter beeinflussen. Üblicherweise werden Faktorladungen bis (je nach Autor) 0.30, bzw. 0.40 nicht beachtet. Außerdem können wir uns die Ausgabe auch sortieren lassen, damit passende Items untereinanderstehen.

```{r}
print.psych(party_efa, cut = 0.40, sort = TRUE)
```

Ein Bild sagt mehr als tausen Worte, deshalb kann man diese Strukturmatrix auch zeichnen.

```{r echo=FALSE}
library(tidytext)

party_efa %>% 
  loadings() %>% 
  unclass() %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Item") %>% 
  rename("Component 1" = TC1, "Component 2" = TC2, "Component 3" = TC3) %>% 
  pivot_longer(
    cols = -Item,
    names_to = "Component",
    values_to = "Loading"
  ) %>%
  mutate(Relevant = if_else(abs(Loading) > .40, TRUE, FALSE),
         Item = reorder_within(Item, Loading, Component)) %>% 
  ggplot(aes(Loading, Item, fill = Relevant)) +
  geom_col() +
  facet_wrap(~ Component, scales = "free_y") +
  scale_y_reordered() +
  scale_fill_personal() +
  theme(legend.position = "bottom")
```

Die Items, die grün dargestellt sind, sind jene mit standardisierten Faktorladungen über .40. Wir haben nun also pro Komponente (oder Faktor) jene Items isoliert, die auf diesem Faktor am meisten laden. Nun müssen wir uns die Beschreibung der Items von oben holen und diese den Faktoren zuordnen.

```{r echo=FALSE}
tribble(
  ~ Text, ~ Faktor,
"Wenn ich an das Wochenende denke, werde ich aufgeregt.", 1,
"Ich mag es, mich mit anderen Menschen zu unterhalten.", 3,
"Mich findet man ab 20:00 Uhr auf der Tanzfläche.", 2,
"Eine spontane Feier wäre jetzt was.", 1,
"Ich könnte die ganze Nacht durchtanzen.", 2,
"Andere Menschen sind mir zuwider.", 3,
"Ich freue mich auf meinen Geburtstag.", 1,
"Ich freue mich auf die Geburtstage meiner Freunde.", 3,
"Ich tanze gerne mit anderen Menschen zusammen.", 2,
"Ich lerne auf Feiern gerne andere Menschen kennen.", 3,
"Two-Step, Air-Punches und Wave sind meine Sprache.", 2,
"Ich organisiere gerne Feiern.", 1,
"Ich freue mich darüber, Einladungen zu Feiern zu erhalten.", 1,
"Ich feiere gerne mit anderen Menschen.", 3,
"Es geht mir gut, wenn ich weiß, dass keine Feier ansteht.", 1
) %>% 
  rownames_to_column(var = "Item") %>% 
  arrange(Faktor) %>% 
  mutate(Faktor = str_c("Faktor", Faktor, sep = " ")) %>% 
  group_by(Faktor) %>% 
  gt()
```

Nun müssen wir selbst tätig werden, indem wir versuchen zu interpretieren, was die Items pro Faktor "gemeinsam" haben. Für den Faktor 1 könnte man sich etwas denken wie "Vorfreude auf eine Feier". Faktor 2 wäre vielleicht etwas wie "Tanzen" und Faktor 3 z.B. "Geselligkeit".

## Interne Konsistenz
Um nun zu überprüfen, ob die gefundenen "Skalen" intern konsistenz sind, können wir Cronbachs $\alpha$ berechnen, das uns angibt, ob alle Items eines Faktors untereinander etwas ähnliches messen.

Dafür erstellen wir uns für jeden Faktor einen Datensatz, der nur die dazugehörigen Variablen enthält.

```{r}
anticipation <- party_recoded %>% 
  select(item_1, item_4, item_7, item_12, item_13, item_15)

dancing <- party_recoded %>% 
  select(item_3, item_5, item_9, item_11)

social <- party_recoded %>% 
  select(item_2, item_6, item_8, item_10, item_14)
```

Für diese einzelnen Datensätze können wir nun Cronbachs $\alpha$ mit der passenden Funktion `alpha()` berechnen. Praktischerweise werden auch direkt Konfidenzintervalle für $\alpha$ angegeben.

```{r}
alpha(anticipation)
alpha(dancing)
alpha(social)
```


# Aus der Praxis
@Nichols.2004 wollten einen Fragebogen zur Internet-Sucht entwickeln. Die Daten ihrer Arbeit sind auf der [begleitenden Website](https://edge.sagepub.com/field5e) von @Field.2018 verfügbar und befinden sich als `nichols_nicki_2004.sav` im Daten-Ordner des [Git-Repositories](https://github.com/benediktclaus/costat/tree/master/data).

In ihrer Analyse der Daten entfernten sie die Items 13, 22, 23, 32 und 34, deswegen werden wir sie hier ebenfalls nicht betrachten. Außerdem sollen die Faktoren nicht korreliert sein, weshalb wir die Varimax-Rotation verwenden.

## EDA

```{r warning=FALSE}
nichols_nicki <- read_spss("data/nichols_nicki_2004.sav") %>% 
  janitor::clean_names() %>% 
  mutate(gender = as_factor(gender))

nichols_nicki_raw <- nichols_nicki %>% 
  select(num_range("ias", 1:36), -ias13, -ias22, -ias23, -ias32, -ias34)

nichols_nicki_raw %>% 
  get_summary_stats()
```

### Bartlett-Test
Der Bartlett-Test ist signifikant, weshalb alles super ist, $K^2(30) = 603.72, p < .001$.

```{r}
bartlett.test(nichols_nicki_raw)
```

### KMO
Der KMO = .94 ist auch wunderbar.

```{r}
KMO(nichols_nicki_raw)
```

## Anzahl der zu extrahierenden Faktoren
### Scree-Test

```{r eval=FALSE}
scree(nichols_nicki_raw, factors = FALSE)
```


```{r echo=FALSE, message=FALSE, results="hide"}
nichols_parallel <- fa.parallel(nichols_nicki_raw, fa = "pc", plot = FALSE)
```

```{r echo=FALSE}
tibble(
  Eigenvalue = nichols_parallel$pc.values
) %>% 
  mutate(factor_number = 1:nrow(.), .before = "Eigenvalue") %>% 
  ggplot(aes(factor_number, Eigenvalue, group = 1)) +
  geom_line() +
  geom_point() +
  labs(x = "Factor Number", y = "Eigenvalue")
```

Gingen wir nach dem Scree-Plot könnte man entweder an einen oder an drei Faktoren denken.

### Parallelanalyse

```{r eval=FALSE}
fa.parallel(nichols_nicki_raw, fa = "pc")
```

```{r echo=FALSE}
tibble(
  Actual = nichols_parallel$pc.values,
  Simulated = nichols_parallel$pc.sim,
  Resampled = nichols_parallel$pc.simr
) %>% 
  mutate(factor_number = 1:nrow(.), .before = Actual) %>% 
  pivot_longer(
    cols = -factor_number,
    names_to = "Sample",
    values_to = "Eigenvalue"
  ) %>% 
  ggplot(aes(factor_number, Eigenvalue, color = Sample)) +
  geom_line() +
  geom_point() +
  scale_color_personal(reverse = TRUE) +
  theme(legend.position = "bottom")
```

Die Parallelanalyse legt uns einen Faktor nahe.

## Durchführung

```{r}
nichols_efa <- principal(nichols_nicki_raw, nfactors = 1, rotate = "varimax")

print.psych(nichols_efa, cut = .3, sort = TRUE)
```

```{r echo=FALSE}
nichols_efa %>% 
  loadings() %>% 
  unclass() %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "item") %>% 
  as_tibble() %>% 
  mutate(item = as_factor(item),
         Relevant = if_else(abs(PC1) > .40, TRUE, FALSE),
         item = fct_reorder(item, PC1)) %>% 
  rename(loadings = PC1) %>% 
  ggplot(aes(x = loadings, item, fill = Relevant)) +
  geom_col() +
  scale_fill_personal(reverse = TRUE) +
  theme(legend.position = "bottom")
```

In diesem Beispiel sind also alle Items für den Faktor relevant. @Nichols.2004 schlossen daraus, dass alle Items einen generellen Faktor in Bezug auf die Internet-Sucht abbilden.

## Interne Konsistenz
Auch Cronbachs $\alpha$ sieht sehr gut aus.

```{r}
alpha(nichols_nicki_raw)
```


# Literatur
